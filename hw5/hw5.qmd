---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 20 @ 11:59PM
author: Zhiyuan Yu 906405523
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
---

## Predicting ICU duration

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

```{r}
library(gtsummary)
library(tidyverse)
library(tidymodels)
library(dplyr)
library(haven)
library(recipes)
library(GGally)
library(ranger)
library(xgboost)
library(stacks)
library(yardstick)
library(purrr)
library(vip)
library(parsnip)
library(tune)
library(dials)
library(purrr)
```
1. Data preprocessing and feature engineering.

**Solution:**
I copied the mimic_icu_cohort.rds file from HW4 to my current working directory of HW5. 
Step 1: Check for missing values
```{r}
mimic_icu_cohort <- read_rds("mimic_icu_cohort.rds") %>% 
  filter(!is.na(los_long)) %>%
  select(-last_careunit, -dod, -discharge_location, -hospital_expire_flag, -los, 
         -intime, -outtime_x, -admittime, -dischtime,
         -deathtime, -edregtime, -edouttime, -outtime_y, -admit_provider_id) %>%
  mutate(
    los_long = as.factor(los_long),
    insurance = as.factor(insurance),
    marital_status = as.factor(marital_status),
    language = as.factor(language)
  )

# Chech for missing values
colSums(is.na(mimic_icu_cohort))
```
Step 2: Take a peek at the data types
```{r}
sapply(mimic_icu_cohort, class)
```
Step 3: Create histograms for the continuous variables to see their distributions
```{r}
# Define a vector with the names of the continuous variables
cont_vars <- c(
  "bicarbonate", "chloride", "creatinine", "glucose", "potassium", "sodium",
  "hematocrit", "white_blood_cells", "heart_rate", 
  "systolic_non_invasive_blood_pressure", 
  "diastolic_non_invasive_blood_pressure", "temperature_fahrenheit", 
  "respiratory_rate", "age_intime"
)

# Reshape the dataset into long format
hist_data_alt <- mimic_icu_cohort %>%
  select(all_of(cont_vars)) %>%
  pivot_longer(
    cols = everything(),
    names_to = "variable",
    values_to = "value"
  )

# Plot the density distributions for each variable
ggplot(hist_data_alt, aes(x = value)) +
  geom_density(kernel = "cosine", fill = "skyblue", alpha = 0.5, na.rm = TRUE) +
  facet_wrap(~ variable, scales = "free") +
  labs(x = "Value", y = "Density") +
  theme_minimal() +
  theme(
    plot.margin = margin(12, 12, 12, 12, "pt"),   
    strip.text = element_text(size = 5),
    axis.text.x = element_text(size = 5),        
    axis.text.y = element_text(size = 5)
  )
```
It is clear that most of the data distributions are either left or right skewed in different levels, therefore I will use median instead of mean to impute the data.

2. Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. Below is the sample code.

**Solution:**
Step 1: split the data into training set and test set
```{r}
#| eval: true
set.seed(203)

# sort the data by subject_id, hadm_id, and stay_id
mimic_icu_cohort <- mimic_icu_cohort |>
  arrange(subject_id, hadm_id, stay_id) |>
  select(-subject_id, -hadm_id, -stay_id)

# partition data into 50% training set and 50% test set
data_split <- initial_split(
  mimic_icu_cohort, 
  strata = "los_long", 
  prop = 0.5
  )

data_split

# check the training set
train_data <- training(data_split)
dim(train_data)

# check the training and testing set
test_data <- testing(data_split) 
dim(test_data)
```
Step 2: Preprocess the data
```{r}
recipe <- recipe(los_long ~ ., data = train_data) %>%
  step_impute_median(all_of(c("bicarbonate", "chloride", "creatinine", 
                              "glucose", "potassium", 
                              "sodium", "hematocrit", "white_blood_cells", 
                              "heart_rate", 
                              "systolic_non_invasive_blood_pressure",                   
                              "diastolic_non_invasive_blood_pressure", 
                              "temperature_fahrenheit", "respiratory_rate", 
                              "age_intime"))) %>%
  step_impute_mode(insurance, marital_status, language) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  print()
```

3. Train and tune the models using the training set.

### First approach: Logistic regression with elastic net regularization

Step 1: Define recipe
```{r}
logit_recipe <- recipe
```

Step 1: Define the logistic regression model for classification and set the engine
```{r}
logit_model <- logistic_reg(
  penalty = tune(),
  mixture = tune()
) %>%
  set_engine("glmnet", standardize = FALSE)

print(logit_model)
```
Step 2: Define workflow
```{r} 
logit_workflow <- workflow() %>%
  add_recipe(logit_recipe) %>%
  add_model(logit_model) %>%
  print()
```
Step 3: Tune the grid and do cross-validation folds
```{r}
logit_param_grid <- grid_regular(
  penalty(range = c(-4, 1)), 
  mixture(),                               
  levels = c(100, 5)) %>%
  print()

set.seed(203)

# define the number of folds for cross-validation is 5
logit_folds <- vfold_cv(train_data, v = 5)
logit_folds
```
Step 4: Fit the corss-validated models and select the best model
```{r}
suppressMessages(suppressWarnings({
  if (file.exists("logit_fit.rds")) {
    logit_fit <- read_rds("logit_fit.rds")
    logit_fit
    
  } else {
    (logit_fit <- logit_workflow |>
      tune_grid(
        resamples = logit_folds,
        grid = logit_param_grid,
        metrics = metric_set(roc_auc, accuracy),
        
        # prepare for later stacking
        control = control_stack_grid()
      )) |>
    system.time()
    
    logit_fit |>
      write_rds("logit_fit.rds")
    
    logit_fit
  }
}))
```

Step 5: Visualize CV result
```{r}
logit_fit |>
  # aggregate metrics from 5-fold cross-validation
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = penalty, y = mean, 
                       color = factor(mixture))) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()
```
Step 6: Show the top 5 models
```{r}
logit_fit %>%
  show_best(metric = "roc_auc")
```
Step 7: Select the best model
```{r}
best_logit <- logit_fit |>
  select_best(metric = "roc_auc")
best_logit
```
Step 8: Finalize model
```{r}
# Final workflow
logit_final_workflow <- logit_workflow %>%
  finalize_workflow(best_logit)
logit_final_workflow

# Fit the whole traning set, then predict the test cases
logit_final_fit <- logit_final_workflow %>%
  last_fit(data_split)
logit_final_fit

# Test metrics
logit_final_fit %>% 
  collect_metrics()
```
Step 9: Plot the variable importance
```{r}
logit_final_fit %>%
  extract_fit_parsnip() %>%
  vip::vip() %>%
  print()
```

### Second Approach: Random Forest
Step 1: Define recipe
```{r}
# define the recipe
rf_recipe <- recipe
```
Step 2: Define Random Forest Model
```{r}
rf_model <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = tune()
) %>%
  set_engine("ranger", importance = "impurity")
rf_model
```
Step 3: Define workflow
```{r}
rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_model) %>%
  print()
```
Step 4: Tuning grid
```{r}
rf_param_grid <- grid_regular(
  trees(range = c(100L, 500L)), 
  mtry(range = c(2L, 5L)),
  levels = c(5, 5)
  )

rf_param_grid
```
Step 5: Cross-validation
```{r}
# Set cross-validation partitions
set.seed(203)

rf_folds <- vfold_cv(train_data, v = 5)
rf_folds

# Fit cross-validation
if (file.exists("rf_fit.rds")) {
  rf_fit <- read_rds("rf_fit.rds")
  rf_fit
  
} else {
  (rf_fit <- rf_workflow %>%
    tune_grid(
      resamples = rf_folds,
      grid = rf_param_grid,
      metrics = metric_set(roc_auc, accuracy),
      control = control_stack_grid()
    )) %>%
  system.time()

  rf_fit %>%
    write_rds("rf_fit.rds")
  
  rf_fit
}
```
Step 6: Visualize CV results
```{r}
rf_fit |>
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = trees, y = mean, 
                       color = factor(mtry))) +
  geom_point() +
  labs(x = "Num. of Trees", y = "CV AUC")
```
Step 7: Show the top 5 models
```{r}
rf_fit |>
  show_best(metric = "roc_auc")
```
Step 8: Select the best model
```{r}
best_rf <- rf_fit |>
  select_best(metric = "roc_auc")
best_rf
```
Step 9: Finalize the model
```{r}
# Final workflow
rf_final_workflow <- rf_workflow %>%
  finalize_workflow(best_rf)
rf_final_workflow

# Fit the whole training set, then predict the test cases
if (file.exists("rf_final_fit.rds")) {
  rf_final_fit <- read_rds("rf_final_fit.rds")
  rf_final_fit
  
} else {
  
  # fit the final model on the whole training set
  rf_final_fit <- rf_final_workflow %>%
    last_fit(data_split)
  
  rf_final_fit %>%
    write_rds("rf_final_fit.rds")
  rf_final_fit
}

# Test metrics
rf_final_fit %>% 
  collect_metrics()
```
Step 10: Plot the variable importance
```{r}
rf_final_fit |>
  extract_fit_engine() |>
  vip::vip() |>
  print()
```

### Third approach: Boosting (XGBoost)
Step 1: Define recipe
```{r}
xgb_recipe <- recipe
```
Step 2: Define boosting model
```{r}
xgb_model <- boost_tree(
  mode = "classification",
  trees = 1000,
  tree_depth = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost")

xgb_model
```
Step 3: Define workflow
```{r}
xgb_workflow <- workflow() %>%
  add_recipe(xgb_recipe) %>%
  add_model(xgb_model) 
xgb_workflow
```
Step 4: Tuning grid
```{r}
xgb_param_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-3, 0), trans = log10_trans()),
  levels = c(3, 3)
  )
xgb_param_grid
```
Step 5: Cross-validation
```{r}
# Set cross-validation partitions
set.seed(203)

xgb_folds <- vfold_cv(train_data, v = 5)
xgb_folds

# Fit cross-validation
if (file.exists("xgb_fit.rds")) {
  xgb_fit <- read_rds("xgb_fit.rds")
  xgb_fit
  
} else {
  (xgb_fit <- xgb_workflow %>%
    tune_grid(
      resamples = xgb_folds,
      grid = xgb_param_grid,
      metrics = metric_set(roc_auc, accuracy),
      control = control_stack_grid()
      )) %>%
    system.time()
  
  xgb_fit %>%
    write_rds("xgb_fit.rds")
  
  xgb_fit
}
```
Step 6: Visualize CV results
```{r}
xgb_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = learn_rate, y = mean, 
                       color = factor(tree_depth))) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```
Step 7: Show the best 5 models
```{r}
xgb_fit |>
  show_best(metric = "roc_auc")
```
Step 8: Select the best model
```{r}
best_xgb <- xgb_fit |>
  select_best(metric = "roc_auc")
best_xgb
```
Step 9: Finalize the model
```{r}
# Final workflow
xgb_final_workflow <- xgb_workflow %>%
  finalize_workflow(best_xgb)
xgb_final_workflow

if (file.exists("xgb_final_fit.rds")) {
  xgb_final_fit <- read_rds("xgb_final_fit.rds")
  xgb_final_fit
  
} else {
  
  # fit the final model on the whole training set
  xgb_final_fit <- xgb_final_workflow %>%
    last_fit(data_split)
  
  xgb_final_fit %>%
    write_rds("xgb_final_fit.rds")
  
  xgb_final_fit
}

# Test metrics
xgb_final_fit |> 
  collect_metrics()
```
Step 10: Plot the variable importance
```{r}
xgb_final_fit %>%
  extract_fit_engine() %>%
  vip::vip() %>%
  print()
```

### Model Stacking
Step 1: Set up the cross-validation folds to be shared by 3 models using in the stack model 
```{r}
set.seed(203)
folds <- vfold_cv(train_data, v = 3)
```

Step 2: Base models. Here we use logistic regression with elastic net regularization, Random Forest, and XGBoosting. To shorten the running time, I changed the grid of each model to smaller numbers to ensure the system won't crash.
```{r}
# Logistic regression with elastic net regularization
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = TRUE)
logit_mod

logit_wf <- workflow() |>
  add_recipe(logit_recipe) |>
  add_model(logit_mod)
logit_wf

logit_stack_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(5, 5)
  )

suppressMessages(suppressWarnings({
  if (file.exists("logit_stack.rds")) {
    logit_stack <- read_rds("logit_stack.rds")
    logit_stack
    
  } else {
    (logit_stack <- logit_wf |>
      tune_grid(
        resamples = folds,
        grid = logit_stack_grid,
        metrics = metric_set(roc_auc, accuracy),
        control = control_stack_grid()
      )) |>
    system.time()
    
    logit_stack |>
      write_rds("logit_stack.rds")
    
    logit_stack
  }
}))
```
```{r}
# Random forest
rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune()
  ) |>
  set_engine("ranger")
rf_mod

rf_wf <- workflow() |>
  add_recipe(rf_recipe) |>
  add_model(rf_mod)
rf_wf

rf_stack_grid <- grid_regular(
  trees(range = c(200L, 500L)), 
  mtry(range = c(1L, 5L)),
  levels = c(5, 2)
  )

if (file.exists("rf_stack.rds")) {
  rf_stack <- read_rds("rf_stack.rds")
  rf_stack
  
} else {
  (rf_stack <- rf_wf %>%
    tune_grid(
      resamples = folds,
      grid = rf_stack_grid,
      metrics = metric_set(roc_auc, accuracy),
      control = control_stack_grid()
    )) %>%
  system.time()

  rf_stack %>%
    write_rds("rf_stack.rds")
  
  rf_stack
}
```
```{r}
# XGBoosting
```


Step 3: Build the stacked ensemble
```{r}

```


4. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each machine learning algorithm and the model stacking. Interpret the results. What are the most important features in predicting long ICU stays? How do the models compare in terms of performance and interpretability?


